{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4. Classification and Clustering on Text Data\n",
    "### Due Date: May 8th (Wednesday) Midnight\n",
    "\n",
    "This week, we will have a look at some classical tasks and you can learn how to apply supervised and unsupervised learning on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Rename your assignment notebook with \"(Your Name)\\_Assignment_X_ ...\" (Not the title, the na)\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> **HTML(.html)**\n",
    "6. Look at the **HTML** file and make sure all your solutions are there, displayed correctly.\n",
    "7. If a file (eg. xxx.json) is required, submit it with the jupyter notebook file and zip them. Rename the zipped file with \"(Your Name)\\_Assignment_X_ ...\"\n",
    "8. Submit your zipped file/notebook on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages you may need to import\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Text Data Classification\n",
    "\n",
    "In this problem we will ues a dataset of movie reviews from IMDb to learn how to perform classification on text data.\n",
    "\n",
    "Download the dataset here: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "The dataset contains the text of the reviews, together with label that marks a review as \"positive\" or \"negative\". In Assignment 2 we have already learnt how to use built-in tools (NLTK, textBlob) to do sentiment analysis. Here we will create a classifier from scratch and you can easily extend the techniques you learned to other advanced tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Load and Preprocess the dataset (30 pts)\n",
    "\n",
    "If you have a look at the unzipped dataset, there is two top-level folders named \"train\" and \"test\" for training and testing data, and in \"train\" folder there are three subfolders for \"pos(itive)\", \"neg(ative)\" and \"unsup(ported)\". For the sake of simplicity, we will treat the training dataset as our whole dataset, resample it to half size and discard data in \"unsup\" folder. \n",
    "\n",
    "Please do following things:\n",
    "\n",
    "1. Delete folder \"unsup\" in \"train\".\n",
    "2. Load data from \"../aclImdb/train\" and store training data into `text_raw` and `y_raw` (sample and target).\n",
    "4. Print out data type of `text_raw` (*list*, *dict* or *tuple*?) and length of it.\n",
    "5. Print out the first element of `text_raw`. Perform reasonale preprocessing on it, and print out the first element of preprocessed text data (named as `text_ready`).\n",
    "6. Split `text_ready` into `X_train_text`, `X_test_text`, `y_train`, `y_test`. The ratio of training data size to `text_ready` size is 0.4, and that of test data is 0.1. Also, the ratio of `pos` to `neg` after split should be the same as that of original data.\n",
    "7. Print out the number of training sample by label (How many for *positive*? How many for *negative*?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_files(r'aclImdb\\\\train')\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = train_data['data']\n",
    "y_raw = train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Preprocessed Text\n",
      "------------------\n",
      "Zero Day leads think even think two boys young men would did commit mutual suicide via slaughtering classmates It captures must beyond bizarre mode being two humans have decided withdraw common civility order define own mutual world via coupled destruction br br It not perfect movie given what money time filmmaker actors remarkable product In terms explaining motives actions the two young suicide murderers is better Elephant terms being film gets our rationalistic skin it is a far far better film almost anything are likely to see br br Flawed honest a terrible honesty\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "text_new = []\n",
    "for i in text_raw:\n",
    "    a = tokenizer.tokenize(i.decode())\n",
    "    text_new.append(a)\n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "for i in text_new:\n",
    "    for j in i:\n",
    "        if j in stop_words:\n",
    "            i.remove(j)\n",
    "\n",
    "text_ready = []\n",
    "for i in text_new:\n",
    "    text_ready.append(' '.join(i))\n",
    "\n",
    "print('------------------')\n",
    "print('Preprocessed Text')\n",
    "print('------------------')\n",
    "print(text_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ready</th>\n",
       "      <th>y_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero Day leads think even think two boys young...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Words t describe bad movie I t explain by writ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everyone plays part pretty well little nice mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There lot highly talented filmmakers actors Ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I evidence confirmed suspicions A bunch kids 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_ready  y_raw\n",
       "0  Zero Day leads think even think two boys young...      1\n",
       "1  Words t describe bad movie I t explain by writ...      0\n",
       "2  Everyone plays part pretty well little nice mo...      1\n",
       "3  There lot highly talented filmmakers actors Ge...      0\n",
       "4  I evidence confirmed suspicions A bunch kids 1...      0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text_ready'] = text_ready\n",
    "df['y_raw'] = y_raw\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Positive samples: 5000\n",
      "The number of Negative samples: 5000\n"
     ]
    }
   ],
   "source": [
    "X_train_text, X_test_text, y_train, y_test = train_test_split(df['text_ready'],\n",
    "                                                             df['y_raw'],\n",
    "                                                              train_size = 0.4,\n",
    "                                                             test_size = 0.1,\n",
    "                                                              stratify = df['y_raw'],\n",
    "                                                             random_state=0)\n",
    "print('The number of Positive samples:',sum(y_train))\n",
    "print('The number of Negative samples:',len(y_train) - sum(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Count-Based Word Vectors (30 pts)\n",
    "\n",
    "One of the challenges for classification on text is how to represent text in a meaningful way. We are familiar with classification on \"numbers\", but how about \"text\"? Do we have a way to retrieve all the information hidden behind text and store them into numbers (vector, matrix or tensor)?\n",
    "\n",
    "One of the most simple but effective way is using *bag-of-words* representation. It is a count-based encoding method, which means we discard most of the structure information on text (like chapters, paragraphs and context), and use \"counts\" (how often each word appears in the text) as the representation.\n",
    "\n",
    "In this problem, we will encode our text data into count-based word vectors. Then we can feed them into our classifier.\n",
    "\n",
    "Please do following things:\n",
    "    \n",
    "1. Use `CountVectorizer` from scikit-learn to produce a vectorizer based on the whole training text data.\n",
    "2. Transform the training and test text data into vectors (numbers) given the vectorizer. Name them as `X_train` and `X_test`.\n",
    "3. Print out how many rows, how many vocabularies (features) and first 20 features. \n",
    "4. Print out a DataFrame whose rows are training samples (sample 0, 1, 2 ...) and columns are features generated by vectorization. (The DataFrame should be very sparse. You can just show `head()` for HTML output)\n",
    "\n",
    "Here is an example:\n",
    "    \n",
    "For corpus: `[\"I love Dartmouth\", \"I love data science\", \"homework is not my love\"]`\n",
    "\n",
    "Your dataframe output should be like this (three rows for three sentences, and columns are \"vocabularies\"):\n",
    "    \n",
    "| * | dartmouth | data | homework | is | love | my | not | science |\n",
    "|:-:|:---------:|:----:|:--------:|:--:|:----:|:--:|:---:|:-------:|\n",
    "| 0 |     1     |   0  |     0    |  0 |   1  |  0 |  0  |    0    |\n",
    "| 1 |     0     |   1  |     0    |  0 |   1  |  0 |  0  |    1    |\n",
    "| 2 |     0     |   0  |     1    |  1 |   1  |  1 |  1  |    0    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "vect = CountVectorizer(ngram_range=(2,3),min_df=5)\n",
    "\n",
    "X_tn = vect.fit(X_train_text)\n",
    "X_tt = vect.fit(X_test_text)\n",
    "\n",
    "X_train = X_tn.transform(X_train_text)\n",
    "X_test = X_tt.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7799)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10 10', '10 because', '10 br', '10 br br', '10 minutes', '10 of', '10 of 10', '10 stars', '10 the', '10 years', '100 years', '12 year', '12 year old', '14 year', '14 year old', '15 minutes', '15 years', '20 minutes', '20 years', '20th century']\n"
     ]
    }
   ],
   "source": [
    "print(X_tn.get_feature_names()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 10</th>\n",
       "      <th>10 because</th>\n",
       "      <th>10 br</th>\n",
       "      <th>10 br br</th>\n",
       "      <th>10 minutes</th>\n",
       "      <th>10 of</th>\n",
       "      <th>10 of 10</th>\n",
       "      <th>10 stars</th>\n",
       "      <th>10 the</th>\n",
       "      <th>10 years</th>\n",
       "      <th>...</th>\n",
       "      <th>your heart</th>\n",
       "      <th>your life</th>\n",
       "      <th>your local</th>\n",
       "      <th>your mind</th>\n",
       "      <th>your money</th>\n",
       "      <th>your own</th>\n",
       "      <th>your seat</th>\n",
       "      <th>your time</th>\n",
       "      <th>zero mostel</th>\n",
       "      <th>zeta jones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 10  10 because  10 br  10 br br  10 minutes  10 of  10 of 10  10 stars  \\\n",
       "0      0           0      0         0           0      0         0         0   \n",
       "1      0           0      0         0           0      0         0         0   \n",
       "2      0           0      0         0           0      0         0         0   \n",
       "3      0           0      0         0           0      0         0         0   \n",
       "4      0           0      0         0           0      0         0         0   \n",
       "\n",
       "   10 the  10 years  ...  your heart  your life  your local  your mind  \\\n",
       "0       0         0  ...           0          0           0          0   \n",
       "1       0         0  ...           0          0           0          0   \n",
       "2       0         0  ...           0          0           0          0   \n",
       "3       0         0  ...           0          0           0          0   \n",
       "4       0         0  ...           0          0           0          0   \n",
       "\n",
       "   your money  your own  your seat  your time  zero mostel  zeta jones  \n",
       "0           0         0          0          0            0           0  \n",
       "1           0         0          0          0            0           0  \n",
       "2           0         0          0          0            0           0  \n",
       "3           0         0          0          0            0           0  \n",
       "4           0         0          0          0            0           0  \n",
       "\n",
       "[5 rows x 7799 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(X_train.todense(),columns=X_tn.get_feature_names())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Classify the Text Data (30 pts)\n",
    "\n",
    "Now we are ready to do the classification:\n",
    "\n",
    "1. Use `LogisticRegression` (param `C` as 0.001) as our classifier.\n",
    "2. Print out cross-validation score (five folds, LR as classifer)\n",
    "3. Use `GridSearchCV` to find out best `C` value among `[0.001, 0.01, 0.1, 1, 10]`. Print out best cross-validation score.\n",
    "4. Do prediction on testset. Print out **precision score**, **recall score** and **f1 score**.\n",
    "5. Draw a **confusion matrix** to show the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.723 , 0.7   , 0.6965, 0.7075, 0.707 ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "LR = LogisticRegression(C=0.001)\n",
    "LR.fit(X_train,y_train)\n",
    "\n",
    "score = cross_val_score(LR, X_train,y_train,cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[0.001,0.01,0.1,1,10]}\n",
    "grid = GridSearchCV(LogisticRegression(),param_grid,cv=5)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7896"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = grid.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.8090062111801242\n",
      "Recall score: 0.8336\n",
      "F1 score: 0.8211189913317573\n",
      "[[1004  246]\n",
      " [ 208 1042]]\n"
     ]
    }
   ],
   "source": [
    "print('Precision score:',precision_score(y_test,predictions))\n",
    "print('Recall score:',recall_score(y_test,predictions))\n",
    "print('F1 score:',f1_score(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Improve your Classifier (20 pts)\n",
    "\n",
    "Let's make some improvements on current classifier. Note that this question is quite open, the grading will depend on how much improvement you can achieve.\n",
    "\n",
    "Leaving blank or no better result will lead to zero point in this question.\n",
    "\n",
    "**Hints**: Context information? Numbers? Stop words? Form of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "# change ngram range from (2,3) to (1,2)\n",
    "# change min_df from 5 to 2 to increase sample size\n",
    "vect = CountVectorizer(ngram_range=(1,2),min_df=2)\n",
    "\n",
    "X_tn = vect.fit(X_train_text)\n",
    "X_tt = vect.fit(X_test_text)\n",
    "\n",
    "X_train = X_tn.transform(X_train_text)\n",
    "X_test = X_tt.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50588)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84 , 0.835, 0.827, 0.825, 0.818])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression(C=0.001)\n",
    "LR.fit(X_train,y_train)\n",
    "\n",
    "score = cross_val_score(LR, X_train,y_train,cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[0.001,0.01,0.1,1,10]}\n",
    "grid = GridSearchCV(LogisticRegression(),param_grid,cv=5)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8732"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = grid.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.8752978554408261\n",
      "Recall score: 0.8816\n",
      "F1 score: 0.8784376245516142\n",
      "[[1093  157]\n",
      " [ 148 1102]]\n"
     ]
    }
   ],
   "source": [
    "print('Precision score:',precision_score(y_test,predictions))\n",
    "print('Recall score:',recall_score(y_test,predictions))\n",
    "print('F1 score:',f1_score(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Topic Modeling and Text Clustering with LDA and KMeans\n",
    "\n",
    "In problem 1 we have learnt a classical example of supervised leaerning (classification). In this problem we are going to look at some unsupervised learning techniques, LDA and KMeans, which play key roles in finding similarity around documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Latent Dirichlet Allocation (20 pts)\n",
    "\n",
    "In some Read It Later or Bookmarking Apps (like Evernote Web Clipper, Pocket, Readability and etc.), one feature is that when you clip some webpage, the app will automatically assign a \"smart category\" or \"smart keywords\" to the page. How does it work?\n",
    "\n",
    "One possible implementation is through Topic Modeling. Let's apply LDA to our movie review dataset to see whether we can find out some \"topics\" hidden behind the reviews.\n",
    "\n",
    "Please do following things:\n",
    "    \n",
    "1. Configure the `CountVectorizer` so that it can remove \"common words\" that appear in at least `15 percent` of the reviews, and also limit feature number to `10000`.\n",
    "2. Vectorize the training data with the configured `CountVectorizer` .\n",
    "3. Use `LatentDirichletAllocation` from scikit-learn to mine the topics. (`10` topics, `batch` as learning method, `30` as max iterations)\n",
    "4. Print out LDA results. Make some comments about the result (Can you name a certain topic?).\n",
    "\n",
    "**Note**: We provide a message printing function named `print_LDA_results()`, where param `lda_model` is your fitted LDA model obejct, `feature_names` are features that build up a certain topic. The default number of supporting features to display is set to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_LDA_results(lda_model, feature_names, n_top_words=15):\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        message = \"Topic %d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=30, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "vect = CountVectorizer(max_df = 0.15, max_features=10000)\n",
    "\n",
    "train_vect = vect.fit(X_train_text)\n",
    "train_vect_t = train_vect.transform(X_train_text)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method='batch', max_iter=30)\n",
    "lda.fit(train_vect_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: worst 10 want horror nothing going actually guy don got pretty funny minutes give thought\n",
      "\n",
      "Topic 1: action fight western james stewart batman john hero bruce martial lee fighting arts kong police\n",
      "\n",
      "Topic 2: music show years funny comedy saw cast lot big always actors fun wonderful songs me\n",
      "\n",
      "Topic 3: we us world war how its real family these things may human fact point other\n",
      "\n",
      "Topic 4: black killer white south school serial high washington tarzan african woods smith jane africa teacher\n",
      "\n",
      "Topic 5: script actors book director nothing poor enough read minutes work any look actually cast seems\n",
      "\n",
      "Topic 6: performance role director cast actor play played performances wife actors work young screen most plays\n",
      "\n",
      "Topic 7: show series episode tv funny original kids new episodes shows old years then children we\n",
      "\n",
      "Topic 8: woman young wife girl gets old father husband new him mother house family goes home\n",
      "\n",
      "Topic 9: horror its effects budget quite work director most dvd however rather more bit set though\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = vect.get_feature_names()\n",
    "print_LDA_results(lda,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: K-Means (20 pts)\n",
    "\n",
    "You may have heard of K-Means algorithm from Machine Learning course. It is such a popular method for unsupervised learning task, partly because its potential advantage on large scale data (easy to parallel). NLTK and Scikit-Learn both provide K-Means implementation, but NLTK has more flexibility on distance metric. Here we will have a look at how to use NLTK version of K-Means.\n",
    "\n",
    "Please do following things:\n",
    "\n",
    "1. Compose `KMeansClusterer` object with `10` as number of target topics, `cosine_distance` from nltk as distance metric and true for `avoid_empty_clusters`.\n",
    "2. Use `CountVectorizer` to create **one-hot encoding** vector representation for `X_train_text`, other params should be same as that of LDA.\n",
    "3. Apply K-Means to vectorized text data (with param `assign_clusters` as True). Print out the first ten topic assignment results.\n",
    "\n",
    "A sample output of printing should be like:\n",
    "\n",
    "```\n",
    "Review: 'b'I registered just to make this comment (which pretty much echos some of the ones here already) The acting is worse than subpar, it expounds on commonly held stereotypes, has some of the worst displays of tasteless female objectification (all bod no brain), and has some of the cheesiest lines known to man.  including but not limited to \"allright lets see what these guys can do\" I should also mention that when they show the crashes involving innocent civilians, you end up feeling bad for the innocent people and start to hate the characters themselves. Eddie Griffin\\'s character is also one of the most stereotypical black guy personas that just rubs people the wrong way. He may or may not be a good actor but this movie doesn\\'t allow for that kind of character exploration. You want a movie that leaves the audience on the side of the bad guys? Oceans 11. This movie just makes you hate the bad guys instead of capturing the audience.  Even the cars can\\'t make up for this fluke of a movie. That Enzo that Griffin wrecked sums up this movie perfectly. It just sucks.'' assigned to cluster 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "from nltk.cluster import cosine_distance\n",
    "kmeans = KMeansClusterer(num_means = 10, distance = cosine_distance, avoid_empty_clusters = True)\n",
    "\n",
    "vectorizer = CountVectorizer(max_df = 0.15, max_features=10000, binary = True)\n",
    "\n",
    "\n",
    "train_vectorized = vectorizer.fit(X_train_text)\n",
    "train_vect1 = train_vectorized.transform(X_train_text)\n",
    "\n",
    "\n",
    "cluster = kmeans.cluster(train_vect1.toarray(), assign_clusters = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:'I registered make comment pretty much echos ones already The acting worse subpar expounds commonly held stereotypes worst displays tasteless female objectification bod brain has some cheesiest lines known man br br including limited allright lets see these guys do I also mention when show crashes involving innocent civilians end feeling bad innocent people start to hate characters Eddie Griffin character also one stereotypical black guy personas rubs people wrong way He may may not be good actor movie t allow kind character exploration You want movie leaves audience the side the bad guys Oceans 11 This movie makes hate the bad guys instead of capturing the audience br br Even the cars t make for this fluke of a movie That Enzo that Griffin wrecked sums this movie perfectly It just sucks' is assigned to cluster 1.\n",
      "\n",
      "Review:'You using IMDb br br You given hefty votes some your favourite films br br It something enjoy br br And s because Fifty seconds One world ends another begins br br How it given ten I wonder those give seven an eight exactly could THE FIRST FILM EVER MADE be better For record long still opening shot great showmanship a superb innovation perfectly suited situation And the dog the bike a lovely touch All this within fifty seconds br br The word genius often overused br br THIS genius' is assigned to cluster 6.\n",
      "\n",
      "Review:'Admittedly I watched piece already VERY low expectations Dieter Bohlen a rather untalented composer parvenu whose lack talent only surpassed size his ego br br This the first cartoon movie I watched 100 per cent humor free It rude offensive redneck blatantly anti women As a creation befitting Bohlen the average viewer be rather put by it No wonder it was never shown a cinema theater It would bombed BIG time br br Not even the expense 6 5 MegaEuros able save utter piece crap Save time money' is assigned to cluster 2.\n",
      "\n",
      "Review:'really loved version Emma best Kate beckinsale awesome Emma mark strong very good knightly only complaint was Mr woodhouse can believe man could whine much be selfish his daughter life was a smart girl end though always love places which Jane Austin movies shot settings spectular makes want visit england so much 9as well Ireland Scotland think actors chosen movie a good choice well all the other story lines interwhined Emma most excellently i am glad i got see this one well' is assigned to cluster 3.\n",
      "\n",
      "Review:'If anyone wondering no one makes movies like used conversation character simple theme friendship struggling evolve something new better different folks need take film see top notch writing directing acting melds wonderful evening observation how things used Italy England Other days times funneled a terrific comedy entertainment made 1992 Alfred Molina Joan Plowright Polly Walker Josie Lawrence Jim Broadbent Miranda Richardson Michael Kitchens major roles Under the brush stroke direction Mike Newell actors accomplish vividly memorable performances are photographed with a sublimely subtle painter eye Reminiscent the theatrical bedroom farce the turn the century this film might be called a friendship farce becomes a worthwhile experience the growth the romantic nature within character the viewer An artistic telegram the importance caring those around us' is assigned to cluster 3.\n",
      "\n",
      "Review:'Cut film film students making film It very much Scream mold ironic self referential horror flick at least falls because for all irony s still a bad horror film the films referring br br But without charms Well one charm anyway Molly Ringwald fantastic as the spoilt bitchy American actress hating every minute working the amateur Australian film crew She so convincing its tempting believe wasn an act although everyone involved Cut says was lovely work br br Seriously every scene her pouting sulking snapping was great Everyone else however wavered being OK being terribly wooden br br Anyway Cut some laughs a few buckets gore some of surprisingly gruesome ultimately just another bad horror film' is assigned to cluster 4.\n",
      "\n",
      "Review:'I saw movie other night I honestly say one worst films I ever seen The acting fair plot totally ridiculous A killer born energy used make movie if film burned killer die How unbelievable that The characters underdeveloped say least example of sudden man mentions Aren trying complete film your mother So re supposed go along We no idea daughter half way the film The movie really t spotlight anyone didn t know anything the main people survived except Ringwald character whiney actress the guy the set the people died Raffy wanted be director like mother Not truly diving know they Seemed things rushed just get the killings The whole plot entirely weak my taste I was extremely disappointed Anyone enjoyed this piece of crap obviously needs learn a thing two film making I t believe anyone would agree to star even work on this picture It funny was not scary was cliche the entire film I found predicting would happen each scene believe you me t hard all to do It s a disgrace I deeply sorry I wasted hour a half watching the mess 1 10' is assigned to cluster 2.\n",
      "\n",
      "Review:'The first time I saw film I loved It different br br I Christian Bible believing I t go along crowd right wing believers I dropped atmosphere br br To in attempts take our government are what Judas tried do I call the Judas Syndrome br br Judas t get even though Jesus said Kingdom not this world br br This film certainly showed of br br I also liked Jesus enjoyed the simple pleasure of playing games jokes his disciples br br Also was a very gorgeous Jesus br br It a watch and movie br br Very satisfying' is assigned to cluster 6.\n",
      "\n",
      "Review:'Okay seems like far Barman fans commented film time counterpoint Beware writeup LONG br br For knowing mostly non Belgians Tom Barman director film frontman dEUS one better known rock bands late 90 here Belgium Basically made couple adventurous innovative albums quickly rose fame national scale Then egos started hurting band basically fell apart Barman couple others remaining go making albums dEUS monicker The way always happens such cases post breakdown dEUS lot tamer less interesting original They tried go international breakthrough their album The Ideal Crash 1999 presenting much diluted form their earlier style songwriting They quite make However egos still pretty big seems big enough Barman consider enough an artist try movies br br More often sort thing VERY big mistake this film make exception And Barman clearly went art this one another very big mistake For one thing musician movie director For another dEUS best made fun provoking music never anything close I would consider art It shows br br So this movie Basically tells story a bunch completely uninteresting people equally uninteresting things course a totally uninteresting friday Antwerp even uninteresting stuff happens them act being uninteresting The characters shallow plot totally pointless film redeeming qualities make shortcomings Humor The whole film made smile slightly 3 times actually managed provoke a single 5 second laugh not quite loud Mood The film doesn seem show kind emotion feeling Mystery Well MINOR SPOILER idea wind man inspiring name the film enthralling a banana pepper pizza not very has done a thousand times anyone remember Johny Destiny one Tarantino s worst appearances film date END MINOR SPOILER And well artistic don t expect kind real action make all the previous In other words except the smiles bored my shorts br br So what remains Well the soundtrack pretty good though suffers some the problems other OST s have shown lately first makes the movie seem like nothing a commercial for the CD Second gives the impression Barman trying hide the weaknesses lack emotional content the film behind the content quality the songs simply doesn t work In the end makes the film look like nothing than an illustration the songs And sadly s Barmans contribution the soundtrack gets the most attention though the weakest part the whole soundtrack far I concerned All all just stands show Barman knows music than movies Camera work okay as well though not anything would make scream out with joy br br The thing about this movie kept watching the sight seeing factor Since I originate Antwerp was fun play a kind guess the location game I would hardly consider this as a quality though br br All all another chance lost for Flemmish film I keep noticing lately the best Belgian movies have been coming the French part the country This is mostly at least have something tell manage tell it in way is sharp emotional the brothers Daerden come mind Maybe the Flemmish art house filmmakers try too' is assigned to cluster 4.\n",
      "\n",
      "Review:'I completely agree comment someone do What up tiger Lily film br br It one the worst french films I seen long time actually along Brotherwood the Wolves 2 horrendous films a much short period time br br It really sad the cast really interesting the original idea kind fun Antoine DeCaunes particular Jean Rochefort among darlings I bitterly disappointed see compromised such a poor film br br Lou Doyon quite bad usual goes prove a pretty face famous parents get into the movies they t necessarily give talent br br avoid this film you want to laugh watch Alain Chabat instead some nice period piece full fun like LA FILLE DE D ARTAGNAN' is assigned to cluster 2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"Review:'{0}' is assigned to cluster {1}.\\n\".format(X_train_text.iloc[i],cluster[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
