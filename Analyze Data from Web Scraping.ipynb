{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3. Analyze Data from Web Scraping\n",
    "### Due Date: April 19th (Friday) Midnight\n",
    "\n",
    "This week, we will obtain and analyze data from web scraping. We will also cover classic NLP metric “TF-IDF” and other useful analyzing metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Text Data from RSS Feed\n",
    "\n",
    "The first problem is about how to obtain content from news feed or blog. You may have heard of some rss reading tools,\n",
    "like the most famous Google Reader (already closed), Reeder app, Feedly, Inoreader and etc. They all allow users to add some their rss feed and the tools will \n",
    "periodly fetch data from the source and show the content to the users after performing some parsing for better reading experience.\n",
    "\n",
    "In this problem, we will develop a toy rss reader. Given a rss feed URL, our reader should extract content from the feed, and provide some linguistic \"metadata\" on the content.\n",
    "\n",
    "Feel free to use propocessing techniques you have learnt in Assignment 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Store content from a RSS Feed (20 pts)\n",
    "\n",
    "The first function of our toy RSS feed is to download data from a RSS feed and make it serializable. We provide a sample RSS feed, Ben Thompson's personal blog **stratechery** (https://stratechery.com/). It's actually a subscription-based newsletter service about technews.\n",
    "\n",
    "You need do following things:\n",
    "\n",
    "- Use `feedparser` to parse the feed\n",
    "- Print out how many entries you fetch from the feed\n",
    "- Discard posts whose title includes \"Exponent Podcast\"; Those are short intros to podcast rather than article\n",
    "- Compose a dict that has keys `title`, `content` and `link` and store corresponding fields of entries in it\n",
    "- Save the dict to a json file called *\"feed.json\"* (serialization)\n",
    "\n",
    "Packages you may use: `feedparser`, `BeautifulSoup`, `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 entries in the feed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "NEWS_FEED = \"http://stratechery.com/feed/\"\n",
    "\n",
    "# write your code here\n",
    "fp = feedparser.parse(NEWS_FEED)\n",
    "print('There are {0} entries in the feed.'.format(len(fp.entries)))\n",
    "\n",
    "index=[]\n",
    "for i in range(len(fp.entries)):\n",
    "    if 'Exponent Podcast' in fp.entries[i]['title']:\n",
    "        index.append(i)\n",
    "index.reverse()\n",
    "for i in index:\n",
    "    del fp.entries[i]\n",
    "\n",
    "dict_rss = {}\n",
    "for i in range(len(fp.entries)):\n",
    "    dict_rss[i]={}\n",
    "    dict_rss[i]['title']=fp.entries[i]['title']\n",
    "    dict_rss[i]['content']=fp.entries[i]['content'][0]['value']\n",
    "    dict_rss[i]['link']=fp.entries[i]['link']\n",
    "\n",
    "with open('feed.json', 'w') as file:\n",
    "    json.dump(dict_rss, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Using NLP tools to process blog data (20 pts)\n",
    "\n",
    "Now we have content data from the blog. The next step is to use NLP tools to analyze the corpus. \n",
    "\n",
    "Please load the json file we stored previously, do following prepocessing:\n",
    "\n",
    "- Remove stop words and punctuation (you can have custom set of punctuation, if necessary)\n",
    "- Tokenize the data with `nltk.tokenize` (which means split the data into sentences, words...)\n",
    "\n",
    "For each post, you need print out:\n",
    "    \n",
    "- The number of sentences \n",
    "- The number of words\n",
    "- The number of unique words\n",
    "- The number of words that only appears only once (hapax legomenon)\n",
    "- Ten most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post1\n",
      "------------------\n",
      "The number of sentences: 62\n",
      "The number of words 1278\n",
      "The number of unique words 697\n",
      "The number of words that only appears only once (hapax legomenon) 490\n",
      "Ten most frequent words [('uber', 60), ('ride-sharing', 17), ('company', 13), ('lyft', 12), ('autonomous', 12), ('the', 11), ('eats', 11), ('revenue', 11), ('i', 11), ('we', 10)]\n",
      "Post2\n",
      "------------------\n",
      "The number of sentences: 87\n",
      "The number of words 1607\n",
      "The number of unique words 786\n",
      "The number of words that only appears only once (hapax legomenon) 514\n",
      "Ten most frequent words [('disney', 47), ('tv', 32), ('content', 26), ('netflix', 21), ('cable', 20), ('business', 19), ('model', 18), ('traditional', 18), ('the', 16), ('sports', 13)]\n",
      "Post3\n",
      "------------------\n",
      "The number of sentences: 64\n",
      "The number of words 1124\n",
      "The number of unique words 661\n",
      "The number of words that only appears only once (hapax legomenon) 482\n",
      "Ten most frequent words [('content', 31), ('free', 20), ('users', 16), ('the', 14), ('youtube', 11), ('facebook', 11), ('providers', 11), ('video', 10), ('also', 9), ('this', 8)]\n",
      "Post4\n",
      "------------------\n",
      "The number of sentences: 80\n",
      "The number of words 1602\n",
      "The number of unique words 815\n",
      "The number of words that only appears only once (hapax legomenon) 542\n",
      "Ten most frequent words [('apple', 114), ('netflix', 22), ('tv', 20), ('i', 18), ('new', 16), ('company', 15), ('services', 14), ('app', 11), ('iphone', 10), ('the', 10)]\n",
      "Post5\n",
      "------------------\n",
      "The number of sentences: 186\n",
      "The number of words 2802\n",
      "The number of unique words 1204\n",
      "The number of words that only appears only once (hapax legomenon) 745\n",
      "Ten most frequent words [('google', 50), ('i', 41), ('facebook', 31), ('apple', 29), ('companies', 26), ('microsoft', 26), ('senator', 25), ('warren', 25), ('tech', 24), ('the', 24)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BLOG_DATA = \"feed.json\"\n",
    "\n",
    "# write your code here\n",
    "with open(BLOG_DATA) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "token_word = [0,0,0,0,0]\n",
    "sentences = [0,0,0,0,0]\n",
    "punctuation = list(string.punctuation) + ['’']\n",
    "stop_words=stopwords.words('english') + punctuation + ['—','’','“','”']\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[str(i)]['content']=BeautifulSoup(data[str(i)]['content'],'html.parser').get_text()\n",
    "    data[str(i)]['content']=data[str(i)]['content'].replace('\\n',' ')\n",
    "    sentences[i]=nltk.sent_tokenize(data[str(i)]['content'])\n",
    "    token_word[i] = word_tokenize(data[str(i)]['content']) \n",
    "    token_word[i] = [w for w in token_word[i] if not w in stop_words]\n",
    "    token_word[i] =[re.sub(r'\\+','',w) for w in token_word[i]]\n",
    "    token_word[i]=[w for w in token_word[i] if not w.isdigit()] # get rid of digits\n",
    "    token_word[i]=[w.lower() for w in token_word[i]] # convert to lowercase in case double count the unique words\n",
    "\n",
    "count_word = [0,0,0,0,0]\n",
    "once = [0,0,0,0,0]\n",
    "for i in range(len(data)):\n",
    "    count_word[i] = Counter(token_word[i])\n",
    "    once[i]=[word for (word, count) in count_word[i].items() if count == 1]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print('Post'+str(i+1)+'\\n------------------')\n",
    "    print('The number of sentences:',len(sentences[i]))\n",
    "    print('The number of words',len(token_word[i]))\n",
    "    print('The number of unique words',len(set(token_word[i])))\n",
    "    print('The number of words that only appears only once (hapax legomenon)',len(once[i]))\n",
    "    print('Ten most frequent words',count_word[i].most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Text Data from Article\n",
    "\n",
    "Not all data sources enable RSS Feed. For those cases, we have to make a call to the target (URL) manually and parse the response by ourselves. \n",
    "\n",
    "For example, Dartmouth News (https://news.dartmouth.edu/): Though it claims it has many feeds (https://www.dartmouth.edu/~dartlife/rss/index.html), no one of them seems working.\n",
    "    \n",
    "In this problem, we will use `boilerpipe` package as the extractor to extract text data directly from an article webpage (https://news.dartmouth.edu/news/2019/01/dartmouth-kicks-its-250th-year-celebration), and execute TF-IDF analysis on the result. `boilerpipe` has built-in rules to handle possible tags in HTML. It uses supervised machine learning to bifurcate the boilerplate and the content of the page, which means, if you provide `boilerpipe` with the response to the call, it can give back the article content auotmatically!\n",
    "\n",
    "Check the paper (https://www.l3s.de/~kohlschuetter/publications/wsdm187-kohlschuetter.pdf) under the hood if you are interested in.\n",
    "\n",
    "You need to install `boilerpipe` with `pip install boilerpipe3`. Since it's just a python wrapper on a java library, you have to install Java environment first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Extract Text from an Article (10 pts)\n",
    "\n",
    "Extract the first three sentences in the article (https://news.dartmouth.edu/news/2019/01/dartmouth-kicks-its-250th-year-celebration) content.\n",
    "\n",
    "Load them into a dictionary named \"corpus\". With keys as 'a', 'b' and 'c' (just simple mark for corpus), the values should be the three sentences.\n",
    "\n",
    "It should be exactly like:\n",
    "    \n",
    "```\n",
    "{'a': 'With speeches, music, and refreshments, Dartmouth launched its yearlong '\n",
    "      '250th anniversary celebration on campus with nine simultaneous kickoffs '\n",
    "      'across the institution on Jan. 10.',\n",
    " 'b': 'Starting at 4 p.m., members of the Dartmouth community gathered at '\n",
    "      'locations across campus to share in the celebration.',\n",
    " 'c': 'Livestreamed on screens across campus from the Dartmouth Library’s '\n",
    "      'Baker-Berry hall, President Philip J. Hanlon ’77 joined 250th co-chairs '\n",
    "      'Cheryl Bascomb ’82 , vice president for alumni relations, and Donald '\n",
    "      'Pease , the Ted and Helen Geisel Third Century Professor in the '\n",
    "      'Humanities, to ring in the sestercentennial.'}\n",
    "```\n",
    "\n",
    "Packages you may use: `boilerpipe`, `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 'With speeches, music, and refreshments, Dartmouth launched its yearlong 250th anniversary celebration on campus with nine simultaneous kickoffs across the institution on Jan. 10.', 'b': 'Starting at 4 p.m., members of the Dartmouth community gathered at locations across campus to share in the celebration.', 'c': 'Livestreamed on screens across campus from the Dartmouth Library’s Baker-Berry hall, President Philip J. Hanlon ’77 joined 250th co-chairs Cheryl Bascomb ’82 , vice president for alumni relations, and Donald Pease , the Ted and Helen Geisel Third Century Professor in the Humanities, to ring in the sestercentennial.'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>With speeches, music, and refreshments, Dartmo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>Starting at 4 p.m., members of the Dartmouth c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>Livestreamed on screens across campus from the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              column\n",
       "a  With speeches, music, and refreshments, Dartmo...\n",
       "b  Starting at 4 p.m., members of the Dartmouth c...\n",
       "c  Livestreamed on screens across campus from the..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import boilerpipe\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://news.dartmouth.edu/news/2019/01/dartmouth-kicks-its-250th-year-celebration\"\n",
    "\n",
    "# write your code here\n",
    "from boilerpipe.extract import Extractor\n",
    "extractor = Extractor(extractor='ArticleExtractor',url=URL)\n",
    "\n",
    "extracted_text = extractor.getText()\n",
    "\n",
    "sentences = nltk.sent_tokenize(extracted_text)\n",
    "\n",
    "corpus = {'a':sentences[9],\n",
    "         'b':sentences[10],\n",
    "         'c':sentences[11]}\n",
    "print(corpus)\n",
    "\n",
    "#convert into dataframe to make life easier for me\n",
    "first_sent = sentences[9]\n",
    "sec_sent = sentences[10]\n",
    "third_sent = sentences[11]\n",
    "corpus_df = pd.DataFrame([first_sent,sec_sent,third_sent],index=['a','b','c'],columns=['column'])\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: TF-IDF analysis on text data (30 pts)\n",
    "\n",
    "TF-IDF stands for term frequency–inverse document frequency and can be used to query a corpus by calculating normalized scores that express the relative importance of terms in the documents.\n",
    "\n",
    "Mathematically, TF-IDF is expressed as the product of the term frequency and the inverse document frequency, $tf-idf = tf*idf$, where the term $tf = \\frac{num\\ of\\ certain\\ word\\ in\\ a\\ sentence}{num\\ of\\ words\\ in\\ a\\ sentence}$ represents the importance of a term in a specific document and $idf = 1 + log(\\frac{num\\ of\\ sentences}{num\\ of\\ sentences\\ that\\ have\\ certain\\ word})$ represents the importance of a term relative to the entire corpus. Multiplying these terms together produces a score that accounts for both factors and has been an integral part of every major search engine at some point in its existence. Here we are using \"sentences\" to denote the corpus in our case. You can extend such definition on paragraphs or even entire documents.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img align=\"center\" src=\"https://s3.amazonaws.com/agwarbliu/TF-IDF.png\" style=\"width: 400px;\" />\n",
    "</p>\n",
    "\n",
    "In this question, you are required to compute TF-IDF on the `corpus` given the `QUERY_WORDS = ['dartmouth', '250th']`.  \n",
    "\n",
    "Specifically, you need to print out these results:\n",
    "\n",
    "- TF score of each sentence in the corpus for each `QUERY_WORDS`\n",
    "- IDF score of entire corpus for each `QUERY_WORDS`\n",
    "- TF-IDF score of each sentence in the corpus for each `QUERY_WORDS`\n",
    "- Viewing the two `QUERY_WORDS` as a vector, sum up their TF-IDF score for each sentence in entire corpus. Compare the result, and make some comments on similarity among sentences.\n",
    "\n",
    "Packages you are allowed to use: `math`, `numpy`. **Please do not use built-in TF-IDF functions in any other packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF score of each sentence in the corpus for each QUERY_WORDS:\n",
      "First sentence with dartmouth: 0.04167\n",
      "Second sentence with dartmouth: 0.05263\n",
      "Third sentence with dartmouth: 0.02128\n",
      "First sentence with 250th: 0.04167\n",
      "Second sentence with 250th: 0.0\n",
      "Third sentence with 250th: 0.02128\n",
      "--------------------\n",
      "IDF score of entire corpus for each QUERY_WORDS:\n",
      "IDF score for dartmouth: 1.0\n",
      "IDF score for 250th: 1.40547\n",
      "--------------------\n",
      "TF-IDF score of each sentence in the corpus for each QUERY_WORDS:\n",
      "First sentence with dartmouth: 0.04167\n",
      "Second sentence with dartmouth: 0.05263\n",
      "Third sentence with dartmouth: 0.02128\n",
      "First sentence with 250th: 0.04167\n",
      "Second sentence with 250th: 0.05263\n",
      "Third sentence with 250th: 0.02128\n",
      "-------------------\n",
      "Sum of TF-IDF score for each sentence in entire corpus:\n",
      "First sentence: 0.10023\n",
      "Second sentence: 0.05263\n",
      "Third sentence: 0.05118\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "QUERY_WORDS = ['dartmouth', '250th']\n",
    "\n",
    "def tf(term, doc, normalize=True):\n",
    "    # write your code here\n",
    "    token_word=[]\n",
    "    token_word = nltk.word_tokenize(doc)\n",
    "    token_word = [w for w in token_word if not w in punctuation] #same punctuation variable from previous question\n",
    "    certain_word = [w for w in token_word if w.lower() == term]\n",
    "    num_certain=np.array(len(certain_word))\n",
    "    num_words=np.array(len(token_word))\n",
    "    tf = np.true_divide(num_certain,num_words)\n",
    "    return tf\n",
    "    \n",
    "def idf(term, corpus):\n",
    "    # write your code here\n",
    "    num_sen=len(corpus_df)\n",
    "    token_word=[0,0,0]\n",
    "    num_certain=0\n",
    "    for i in range(len(corpus_df)):\n",
    "        token_word[i] = nltk.word_tokenize(corpus_df['column'][i])\n",
    "        token_word[i] = [w for w in token_word[i] if not w in punctuation]\n",
    "        for w in token_word[i]:\n",
    "            if w.lower() == term:\n",
    "                num_certain += 1\n",
    "    idf = 1+log(np.true_divide(num_sen,num_certain))\n",
    "    return idf\n",
    "    \n",
    "def tf_idf(term, doc, corpus):\n",
    "    # write your code here\n",
    "    \n",
    "    return tf(term, doc) * idf(term, corpus)\n",
    "\n",
    "print('TF score of each sentence in the corpus for each QUERY_WORDS:')\n",
    "print('First sentence with dartmouth:',round(tf(QUERY_WORDS[0],corpus_df['column'][0]),5))\n",
    "print('Second sentence with dartmouth:',round(tf(QUERY_WORDS[0],corpus_df['column'][1]),5))\n",
    "print('Third sentence with dartmouth:',round(tf(QUERY_WORDS[0],corpus_df['column'][2]),5))\n",
    "print('First sentence with 250th:',round(tf(QUERY_WORDS[1],corpus_df['column'][0]),5))\n",
    "print('Second sentence with 250th:',round(tf(QUERY_WORDS[1],corpus_df['column'][1]),5))\n",
    "print('Third sentence with 250th:',round(tf(QUERY_WORDS[1],corpus_df['column'][2]),5))\n",
    "print('--------------------')\n",
    "print('IDF score of entire corpus for each QUERY_WORDS:')\n",
    "print('IDF score for dartmouth:',round(idf(QUERY_WORDS[0],corpus_df),5))\n",
    "print('IDF score for 250th:',round(idf(QUERY_WORDS[1],corpus_df),5))\n",
    "print('--------------------')\n",
    "print('TF-IDF score of each sentence in the corpus for each QUERY_WORDS:')\n",
    "print('First sentence with dartmouth:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][0], corpus),5))\n",
    "print('Second sentence with dartmouth:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][1], corpus),5))\n",
    "print('Third sentence with dartmouth:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][2], corpus),5))\n",
    "print('First sentence with 250th:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][0], corpus),5))\n",
    "print('Second sentence with 250th:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][1], corpus),5))\n",
    "print('Third sentence with 250th:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][2], corpus),5))\n",
    "print('-------------------')\n",
    "print('Sum of TF-IDF score for each sentence in entire corpus:')\n",
    "print('First sentence:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][0], corpus_df)+tf_idf(QUERY_WORDS[1], corpus_df['column'][0], corpus_df),5))\n",
    "print('Second sentence:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][1], corpus_df)+tf_idf(QUERY_WORDS[1], corpus_df['column'][1], corpus_df),5))\n",
    "print('Third sentence:',round(tf_idf(QUERY_WORDS[0], corpus_df['column'][2], corpus_df)+tf_idf(QUERY_WORDS[1], corpus_df['column'][2], corpus_df),5)) \n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, the first sentence in the corpus has the highest frequency of the two QUERY_WORDS while the second and third sentence have about the same frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Chart Data from Webpage (20 pts)\n",
    "\n",
    "Sometimes you may meet data in charts. In this problem, you need write code to download the weekend box office chart from [Rotten Tomatoes](https://www.rottentomatoes.com/browse/box-office/), parse the web page, load the chart to pandas dataframe, and print out the chart.\n",
    "\n",
    "Packages you may use: `requests`, `pandas`, `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>THIS WEEK</th>\n",
       "      <th>LAST WEEK</th>\n",
       "      <th>T-METER</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>WEEKS RELEASED</th>\n",
       "      <th>WEEKEND GROSS</th>\n",
       "      <th>TOTAL GROSS</th>\n",
       "      <th>THEATER AVERAGE</th>\n",
       "      <th># OF THEATERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>--</td>\n",
       "      <td>33%</td>\n",
       "      <td>The Curse of La Llorona</td>\n",
       "      <td>1</td>\n",
       "      <td>$26.5M</td>\n",
       "      <td>$26.5M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>90%</td>\n",
       "      <td>Shazam!</td>\n",
       "      <td>3</td>\n",
       "      <td>$17.3M</td>\n",
       "      <td>$17.3M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>--</td>\n",
       "      <td>64%</td>\n",
       "      <td>Breakthrough</td>\n",
       "      <td>1</td>\n",
       "      <td>$11.1M</td>\n",
       "      <td>$11.1M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>78%</td>\n",
       "      <td>Captain Marvel</td>\n",
       "      <td>7</td>\n",
       "      <td>$9.1M</td>\n",
       "      <td>$9.1M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>46%</td>\n",
       "      <td>Little</td>\n",
       "      <td>2</td>\n",
       "      <td>$8.5M</td>\n",
       "      <td>$8.5M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>47%</td>\n",
       "      <td>Dumbo</td>\n",
       "      <td>4</td>\n",
       "      <td>$6.8M</td>\n",
       "      <td>$6.8M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>58%</td>\n",
       "      <td>Pet Sematary</td>\n",
       "      <td>3</td>\n",
       "      <td>$4.9M</td>\n",
       "      <td>$4.9M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>89%</td>\n",
       "      <td>Missing Link</td>\n",
       "      <td>2</td>\n",
       "      <td>$4.4M</td>\n",
       "      <td>$4.4M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>94%</td>\n",
       "      <td>Us</td>\n",
       "      <td>5</td>\n",
       "      <td>$4.3M</td>\n",
       "      <td>$4.3M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>14%</td>\n",
       "      <td>Hellboy</td>\n",
       "      <td>2</td>\n",
       "      <td>$3.9M</td>\n",
       "      <td>$3.9M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>21%</td>\n",
       "      <td>After</td>\n",
       "      <td>2</td>\n",
       "      <td>$2.5M</td>\n",
       "      <td>$2.5M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>--</td>\n",
       "      <td>90%</td>\n",
       "      <td>Disneynature: Penguins</td>\n",
       "      <td>1</td>\n",
       "      <td>$2.4M</td>\n",
       "      <td>$2.4M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>--</td>\n",
       "      <td>33%</td>\n",
       "      <td>Kalank</td>\n",
       "      <td>1</td>\n",
       "      <td>$1.3M</td>\n",
       "      <td>$1.3M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>91%</td>\n",
       "      <td>How to Train Your Dragon: The Hidden World</td>\n",
       "      <td>9</td>\n",
       "      <td>$0.8M</td>\n",
       "      <td>$0.8M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>48%</td>\n",
       "      <td>Unplanned</td>\n",
       "      <td>4</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>55%</td>\n",
       "      <td>Five Feet Apart</td>\n",
       "      <td>6</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>52%</td>\n",
       "      <td>The Best of Enemies</td>\n",
       "      <td>3</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>99%</td>\n",
       "      <td>Amazing Grace</td>\n",
       "      <td>3</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>$0.6M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>94%</td>\n",
       "      <td>The Mustang</td>\n",
       "      <td>6</td>\n",
       "      <td>$0.5M</td>\n",
       "      <td>$0.5M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>75%</td>\n",
       "      <td>Hotel Mumbai</td>\n",
       "      <td>4</td>\n",
       "      <td>$0.5M</td>\n",
       "      <td>$0.5M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>33%</td>\n",
       "      <td>Wonder Park</td>\n",
       "      <td>6</td>\n",
       "      <td>$0.5M</td>\n",
       "      <td>$0.5M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>83%</td>\n",
       "      <td>High Life</td>\n",
       "      <td>3</td>\n",
       "      <td>$0.3M</td>\n",
       "      <td>$0.3M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>70%</td>\n",
       "      <td>Teen Spirit</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.2M</td>\n",
       "      <td>$0.2M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>13%</td>\n",
       "      <td>Tyler Perry's A Madea Family Funeral</td>\n",
       "      <td>8</td>\n",
       "      <td>$0.2M</td>\n",
       "      <td>$0.2M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>98%</td>\n",
       "      <td>Apollo 11</td>\n",
       "      <td>8</td>\n",
       "      <td>$0.1M</td>\n",
       "      <td>$0.1M</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>39%</td>\n",
       "      <td>The Chaperone</td>\n",
       "      <td>4</td>\n",
       "      <td>$86.0k</td>\n",
       "      <td>$86.0k</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>91%</td>\n",
       "      <td>Gloria Bell</td>\n",
       "      <td>5</td>\n",
       "      <td>$76.0k</td>\n",
       "      <td>$76.0k</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>14%</td>\n",
       "      <td>No Manches Frida 2</td>\n",
       "      <td>6</td>\n",
       "      <td>$70.0k</td>\n",
       "      <td>$70.0k</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>42</td>\n",
       "      <td>85%</td>\n",
       "      <td>Her Smell</td>\n",
       "      <td>2</td>\n",
       "      <td>$69.0k</td>\n",
       "      <td>$69.0k</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>91%</td>\n",
       "      <td>Wild Nights with Emily</td>\n",
       "      <td>2</td>\n",
       "      <td>$68.0k</td>\n",
       "      <td>$68.0k</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    THIS WEEK LAST WEEK T-METER                                       TITLE  \\\n",
       "0           1        --     33%                     The Curse of La Llorona   \n",
       "1           2         1     90%                                     Shazam!   \n",
       "2           3        --     64%                                Breakthrough   \n",
       "3           4         6     78%                              Captain Marvel   \n",
       "4           5         2     46%                                      Little   \n",
       "5           6         5     47%                                       Dumbo   \n",
       "6           7         4     58%                                Pet Sematary   \n",
       "7           8         9     89%                                Missing Link   \n",
       "8           9         7     94%                                          Us   \n",
       "9          10         3     14%                                     Hellboy   \n",
       "10         11         8     21%                                       After   \n",
       "11         12        --     90%                      Disneynature: Penguins   \n",
       "12         13        --     33%                                      Kalank   \n",
       "13         14        16     91%  How to Train Your Dragon: The Hidden World   \n",
       "14         15        11     48%                                   Unplanned   \n",
       "15         16        12     55%                             Five Feet Apart   \n",
       "16         17        10     52%                         The Best of Enemies   \n",
       "17         18        17     99%                               Amazing Grace   \n",
       "18         19        15     94%                                 The Mustang   \n",
       "19         20        13     75%                                Hotel Mumbai   \n",
       "20         21        14     33%                                 Wonder Park   \n",
       "21         22        21     83%                                   High Life   \n",
       "22         23        39     70%                                 Teen Spirit   \n",
       "23         24        18     13%        Tyler Perry's A Madea Family Funeral   \n",
       "24         25        25     98%                                   Apollo 11   \n",
       "25         26        35     39%                               The Chaperone   \n",
       "26         27        24     91%                                 Gloria Bell   \n",
       "27         28        22     14%                          No Manches Frida 2   \n",
       "28         29        42     85%                                   Her Smell   \n",
       "29         30        45     91%                      Wild Nights with Emily   \n",
       "\n",
       "    WEEKS RELEASED WEEKEND GROSS TOTAL GROSS THEATER AVERAGE # OF THEATERS  \n",
       "0                1        $26.5M      $26.5M              --            --  \n",
       "1                3        $17.3M      $17.3M              --            --  \n",
       "2                1        $11.1M      $11.1M              --            --  \n",
       "3                7         $9.1M       $9.1M              --            --  \n",
       "4                2         $8.5M       $8.5M              --            --  \n",
       "5                4         $6.8M       $6.8M              --            --  \n",
       "6                3         $4.9M       $4.9M              --            --  \n",
       "7                2         $4.4M       $4.4M              --            --  \n",
       "8                5         $4.3M       $4.3M              --            --  \n",
       "9                2         $3.9M       $3.9M              --            --  \n",
       "10               2         $2.5M       $2.5M              --            --  \n",
       "11               1         $2.4M       $2.4M              --            --  \n",
       "12               1         $1.3M       $1.3M              --            --  \n",
       "13               9         $0.8M       $0.8M              --            --  \n",
       "14               4         $0.6M       $0.6M              --            --  \n",
       "15               6         $0.6M       $0.6M              --            --  \n",
       "16               3         $0.6M       $0.6M              --            --  \n",
       "17               3         $0.6M       $0.6M              --            --  \n",
       "18               6         $0.5M       $0.5M              --            --  \n",
       "19               4         $0.5M       $0.5M              --            --  \n",
       "20               6         $0.5M       $0.5M              --            --  \n",
       "21               3         $0.3M       $0.3M              --            --  \n",
       "22               1         $0.2M       $0.2M              --            --  \n",
       "23               8         $0.2M       $0.2M              --            --  \n",
       "24               8         $0.1M       $0.1M              --            --  \n",
       "25               4        $86.0k      $86.0k              --            --  \n",
       "26               5        $76.0k      $76.0k              --            --  \n",
       "27               6        $70.0k      $70.0k              --            --  \n",
       "28               2        $69.0k      $69.0k              --            --  \n",
       "29               2        $68.0k      $68.0k              --            --  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# write your code here\n",
    "url = \"https://www.rottentomatoes.com/browse/box-office/\"\n",
    "response=requests.get(url)\n",
    "html=response.content\n",
    "tables = pd.read_html(html)\n",
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
